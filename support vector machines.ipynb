{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM5vnNaHzyWNqI7EH9zvwMU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# SMV Theory\n","\n","## Contexto historico:\n","\n","Fueron desarrolladas principalmente por Vladimir Vapnik en los años 90s en la unión soviética, quien fue por mucho tiempo profesor en una de las universidades soviética hasta que en el año 1990 emigró a estados unidos donde trabajó en AT&T en este periodo en la década de los 90s cuando desarrolló esta idea de la máquina de soporte vectorial que se volvió muy importante no sólo la técnica en sí sino también toda la teoría que se desarrolló alrededor de esta técnica entonces aparte de la SVM es importante conocer que también a partir de ella se desarrolló la teoría de Vapnik-Chervonenkis del aprendizaje estadístico aquí pueden ver la imagen a Vapnik leyendo su propio libro sobre la teoría de aprendizaje estadístico."],"metadata":{"id":"_shRU9FsR0Dg"}},{"cell_type":"markdown","source":["![Screenshot 2023-05-15 235357](https://github.com/DataEngel/scaffold/assets/63415652/161651d9-5630-405e-a175-d267bcd57764)"],"metadata":{"id":"sr0Zw-ZYINA4"}},{"cell_type":"markdown","source":["Las SVM la pueden encontrar para estos tres propósitos: \n","\n","- Clasificación.\n","- Regresión.\n","- Clustering.\n","\n","El principal es el problema de clasificación lo vamos a ver de esta manera para no tener que ver cada uno de estos problemas y lo vamos a ver como un problema de clasificador pero hay algunas extensiones para volverlo un regresor con truco muy parecido a lo que hace cada KNN y hay otro truco para convertirlo a un algoritmo de clustering que es no supervisado que fue publicado por Vapnik y otro científico en el 2001, pero solo vamos a ver el clasificador para entender las inspiraciones de la SVM y vamos a partir con un problema de clasificación."],"metadata":{"id":"BtXyfKZqHeN5"}},{"cell_type":"markdown","source":["## Hiperplano optimo.\n","\n","Lo que buscamos es maximizar la distancia de nuestro hiperplano, a esto le llaman el método de la calle porque parece un poco una calle donde tenemos la línea del medio que divide los carriles y los bordes de nuestra carretera, entonces Vapnik tuvo la idea de que la línea óptima que separa estas dos clases y por lo tanto la que tiene mejor generalización, es decir la que lo hace mejor para puntos patrones que nunca ha visto va a ser esta línea de aquí, donde el margen sea el máximo posible se busca maximizar el margen de aquí. Entonces, vamos a desarrollar con esta idea que es muy sencilla las matemáticas que Vapnik creó.\n","\n","\n","![1](https://github.com/DataEngel/Data-Science-Machine-Learning-Articles/assets/63415652/d8678645-a821-4db8-b564-6a728dc09aeb)"],"metadata":{"id":"3CNm0l6JOCbw"}},{"cell_type":"markdown","source":["Esta línea tiene parámetros $w$ entonces voy a poner $w$ como un vector que va a ser $w_1$ y $w_2$ y también tiene parámetros $b$, si vemos $w$ está como un vector va a ser un vector, entonces voy a estirar esta línea que va a ser perpendicular a nuestra línea eso se cumple siempre si yo tengo los parámetros $w$ los parámetros no independientes de una línea de un híper plano me van a formar un vector que va a apuntar de manera normal a intercalar el hyperplane, esto es muy importante porque vamos a usar ese vector $w$ que acabamos de escribir para describir cualquier otro punto.   \n","\n","Entonces si yo agarro otro punto vamos a pensar un punto por aquí le voy a llamar $x$ pensándolo que es un vector, estas líneas son rectas acabo de dibujar vamos a tener un vector $w$ y un vector $x$, cómo puedo describir ese vector $x$ con respecto a $w$.\n","\n","![Screenshot 2023-05-18 225625](https://github.com/DataEngel/Data-Science-Machine-Learning-Articles/assets/63415652/2045654b-2699-4d25-8f08-578dbea6bb49)"],"metadata":{"id":"b78aJ-ZlRPwP"}},{"cell_type":"markdown","source":["Pues lo puedo describir con la proyección es decir la sombra de este vector sobre $w$ vale y bueno yo sé que esa proyección realmente sería $x$ producto punto de $w$ entre la norma de $w$. Esta proyección va pintar de otro conocido esta proyección de aquí entonces muy importante porque los productos puntos nos van a ayudar a proyectar cualquier punto en dirección hacia el hiper plano en una dirección normal al hiper plano.  \n","\n","### Proyectando una muestra. \n","\n","![2](https://github.com/DataEngel/Data-Science-Machine-Learning-Articles/assets/63415652/3410cb28-e87d-4c0e-950a-29b798d5c43d)"],"metadata":{"id":"rqwR-tME06Vw"}},{"cell_type":"markdown","source":["Entonces yo voy a decir que pertenece a una clase positiva esta de aquí si la proyección que $x \\cdot { \\frac{w}{||w||} }$ es una proyección con un factor, si eso es mayor a cierta constante voy a decir que pertenece a la clase positiva, a esta clase positiva y si es menor si esa ecuación booleana a me da negativo pues va a pertenecer parte negativa tomando en cuenta el sesgo podemos llegar a esta ecuación que es la misma ecuación de la línea, hay que recordar que aquí $x$ es un vector que pertenece a este espacio y $w$ es otro vector que pertenece a este espacio todo su producto punto es un escalar. Sumado un escalar lo puedo comparar con otra escala entonces recordando un poquito la ecuación de la línea la ecuación de esta línea es $w \\cdot x + b = 0$ todo lo que sea mayor a 0 va a pertenecer de este lado del el plano y todo lo que sea menor a 0 va a pertenecer de este lado de libertad\n","\n","$w^Tx$ lo estoy utilizando con el producto punto para enfatizar un poco que esto es la proyección, este vector, es la proyección pero lo que yo quiero realmente es encontrar puntos que estén fuera de estos márgenes no simplemente que pasen el hiper plano sino que también estén alejados del hiper plano por cierta dimensión. \n","\n","Vamos a recordar la ecuación para calcular la distancia de un punto hacia una línea vale de un punto hacia un hiper plano entonces recordando un poquito para una línea vamos a verlo de esta manera para una línea $ax+by+c$ vale la distancia a un punto la distancia de la línea al punto  viene dada por esta misma ecuación $d(l,p)= \\frac{ax+by+c}{\\sqrt{a^2+b^2}}$ esta ecuación para un punto con coordenadas $(x,y)$ una línea con parámetros **a b** y **c** esta vez es la distancia, ojo aquí la distancia depende de los parámetros de mi línea, entonces yo podría poner una distancia ficticia a este margen que puedo llamarle un 1, esa distancia se va a escalar entonces no importa mucho el dígito que yo ponga por simplicidad voy a poner que en esta métrica esa distancia me va a valer 1.\n","\n","\n","![3](https://github.com/DataEngel/Data-Science-Machine-Learning-Articles/assets/63415652/c467a3ed-4287-467f-a3a4-d7341f861ad4)\n"],"metadata":{"id":"Y5Dk8C5E1isk"}},{"cell_type":"markdown","source":["Vamos a verlo en ecuaciones entonces la ecuación que nosotros teníamos esta de aquí la vamos a cambiar porque ya no quiero simplemente que esté de un lado del híper plano o del otro sino que también esté a cierto margen entonces en lugar de que sea cero que es el lugar geométrico donde vive esta línea vale el híper plano azul pues lo que voy a poner 1.  "],"metadata":{"id":"crWK1G9N16UI"}},{"cell_type":"markdown","source":["## Reglas de decision:\n","\n","Para muestras positivas se tiene que dar una razón mayor que uno, qué significa eso? que de mi hiper plano y de su margen derecho esta muestra va a estar para la derecha, si esta muestra es mayor que uno quiere decir que estoy fuera del margen que estoy más allá del margen y para una muestra negativa ocurrió exactamente lo contrario yo creo que esté a menos 1 de ese hiper plano, entonces estas ecuaciones se cumplen siempre que sepamos que las muestras son positivas o si son negativas. \n","\n","- Agreguemos una restricción la problema.\n","    - $w \\cdot{x_x+b \\geq{1}}$\n","    - $w\\cdot{x_-+b\\leq{-1}}$\n","\n","Una forma de generalizar esto es poniendo el valor deseado para cada muestra entonces el valor deseado me dice si quiero que sea parte de los positivos que me dé 1 si $x_1$ es una muestra positiva o que me dé menos 1 si $x_1$ es una muestra negativa, lo que estamos haciendo ahorita es simplemente una clasificación binaria entonces puedo generalizar estas dos ecuaciones con esta: \n","\n","- Juntamos las ecuaciones:\n","\n","$y_i= \\left\\{\n","\\begin{array}{ l }\n","1 \\:\\: Si \\:\\: x_i \\:\\: es \\:\\: + \\\\\n","-1 \\:\\: Si \\:\\: x_i \\:\\:es \\:\\: -\n","\\end{array}\n","\\right.$ -------->  $y_1(w\\cdot{x_i}+b)\\geq{1}$\n","\n","\n","Entonces para todos los puntos vale **x** van a cumplir esta manera ecuación si están fuera del margen, la idea de Vapnik es encontrar la línea con este margen bilateral tal que ningún punto caiga dentro de este margen, si hay un punto aquí hay que hacer el margen más pequeño, entonces hay que tratar de hacer el margen lo más grande posible tal que no haya ningún punto que esté dentro y eso va a ser condicionado por estos puntos que van a estar en las en los márgenes y de hecho a estos puntos reciben un nombre especial que son vectores soportes como se imaginarán pueda técnica significa máquinas de vector soporte entonces va a ser una máquina de estos vectores que tenemos que encontrar todo va a depender de estos vectores que van a estar en la periferia del margen bien entonces esta ecuación que hemos desarrollado hasta ahorita nos asegura que un punto de $(x,y)$ va a estar fuera del margen que queremos poner,.\n","\n","Ahora, qué pasa si yo quiero los vectores soportes no cualquier patrón sino los vectores soportes en el caso de los vectores soportes no solo se va a cumplir esta ecuación sino que se va a cumplir con la igualdad.  \n","\n","Entonces despejando la ecuación anterior voy a pasar el uno al otro lado con signo negativo vale y para las muestras que son los vectores soporte se va a cumplir en la igualdad vale es decir estos vectores que están en la periferia del margen van a cumplir esa igualdad y todos los que estén más allá van a cumplir la desigualdad.\n","\n","Entonces a partir de los parámetros de la línea hemos llegado a una ecuación que me detecta este esos vectores soportes vale todos los vectores que sean soporte van a cumplir esa ecuación de ahí. \n","\n","- Despejando la ecuación:\n","    - $y_i(w\\cdot{x_i}+b)\\geq1$\n","    - $y_i(w\\cdot{x_i}+b)-1\\geq0$\n","- En el caso especial de muestras x que estén en el margen.\n","    - $y_i(w\\cdot{x_i}+b)-1=0$"],"metadata":{"id":"LbGAKfOLFXLV"}},{"cell_type":"markdown","source":[],"metadata":{"id":"pSgWQozWFXkb"}},{"cell_type":"markdown","source":[],"metadata":{"id":"Y2qN5Wc5FXm4"}},{"cell_type":"markdown","source":[],"metadata":{"id":"o6ZOmbPlFXpp"}}]}