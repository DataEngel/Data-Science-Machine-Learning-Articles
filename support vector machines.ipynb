{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM9ACkcL/xPtdxRfPfZ8weU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# SMV Theory\n","\n","## Contexto historico:\n","\n","Fueron desarrolladas principalmente por Vladimir Vapnik en los años 90s en la unión soviética, quien fue por mucho tiempo profesor en una de las universidades soviética hasta que en el año 1990 emigró a estados unidos donde trabajó en AT&T en este periodo en la década de los 90s cuando desarrolló esta idea de la máquina de soporte vectorial que se volvió muy importante no sólo la técnica en sí sino también toda la teoría que se desarrolló alrededor de esta técnica entonces aparte de la SVM es importante conocer que también a partir de ella se desarrolló la teoría de Vapnik-Chervonenkis del aprendizaje estadístico aquí pueden ver la imagen a Vapnik leyendo su propio libro sobre la teoría de aprendizaje estadístico."],"metadata":{"id":"_shRU9FsR0Dg"}},{"cell_type":"markdown","source":["![Screenshot 2023-05-15 235357](https://github.com/DataEngel/scaffold/assets/63415652/161651d9-5630-405e-a175-d267bcd57764)"],"metadata":{"id":"sr0Zw-ZYINA4"}},{"cell_type":"markdown","source":["Las SVM la pueden encontrar para estos tres propósitos: \n","\n","- Clasificación.\n","- Regresión.\n","- Clustering.\n","\n","El principal es el problema de clasificación lo vamos a ver de esta manera para no tener que ver cada uno de estos problemas y lo vamos a ver como un problema de clasificador pero hay algunas extensiones para volverlo un regresor con truco muy parecido a lo que hace cada KNN y hay otro truco para convertirlo a un algoritmo de clustering que es no supervisado que fue publicado por Vapnik y otro científico en el 2001, pero solo vamos a ver el clasificador para entender las inspiraciones de la SVM y vamos a partir con un problema de clasificación."],"metadata":{"id":"BtXyfKZqHeN5"}},{"cell_type":"markdown","source":["## Hiperplano optimo.\n","\n","Lo que buscamos es maximizar la distancia de nuestro hiperplano, a esto le llaman el método de la calle porque parece un poco una calle donde tenemos la línea del medio que divide los carriles y los bordes de nuestra carretera, entonces Vapnik tuvo la idea de que la línea óptima que separa estas dos clases y por lo tanto la que tiene mejor generalización, es decir la que lo hace mejor para puntos patrones que nunca ha visto va a ser esta línea de aquí, donde el margen sea el máximo posible se busca maximizar el margen de aquí. Entonces, vamos a desarrollar con esta idea que es muy sencilla las matemáticas que Vapnik creó.\n","\n","\n","![1](https://github.com/DataEngel/Data-Science-Machine-Learning-Articles/assets/63415652/d8678645-a821-4db8-b564-6a728dc09aeb)"],"metadata":{"id":"3CNm0l6JOCbw"}},{"cell_type":"markdown","source":["Esta línea tiene parámetros $w$ entonces voy a poner $w$ como un vector que va a ser $w_1$ y $w_2$ y también tiene parámetros $b$, si vemos $w$ está como un vector va a ser un vector, entonces voy a estirar esta línea que va a ser perpendicular a nuestra línea eso se cumple siempre si yo tengo los parámetros $w$ los parámetros no independientes de una línea de un híper plano me van a formar un vector que va a apuntar de manera normal a intercalar el hyperplane, esto es muy importante porque vamos a usar ese vector $w$ que acabamos de escribir para describir cualquier otro punto.   \n","\n","Entonces si yo agarro otro punto vamos a pensar un punto por aquí le voy a llamar $x$ pensándolo que es un vector, estas líneas son rectas acabo de dibujar vamos a tener un vector $w$ y un vector $x$, cómo puedo describir ese vector $x$ con respecto a $w$.\n","\n","![Screenshot 2023-05-18 225625](https://github.com/DataEngel/Data-Science-Machine-Learning-Articles/assets/63415652/2045654b-2699-4d25-8f08-578dbea6bb49)"],"metadata":{"id":"b78aJ-ZlRPwP"}},{"cell_type":"markdown","source":["Pues lo puedo describir con la proyección es decir la sombra de este vector sobre $w$ vale y bueno yo sé que esa proyección realmente sería $x$ producto punto de $w$ entre la norma de $w$. Esta proyección va pintar de otro conocido esta proyección de aquí entonces muy importante porque los productos puntos nos van a ayudar a proyectar cualquier punto en dirección hacia el hiper plano en una dirección normal al hiper plano.  \n","\n","### Proyectando una muestra. \n","\n","![2](https://github.com/DataEngel/Data-Science-Machine-Learning-Articles/assets/63415652/3410cb28-e87d-4c0e-950a-29b798d5c43d)"],"metadata":{"id":"rqwR-tME06Vw"}},{"cell_type":"markdown","source":["Entonces yo voy a decir que pertenece a una clase positiva esta de aquí si la proyección que $x \\cdot { \\frac{w}{||w||} }$ es una proyección con un factor, si eso es mayor a cierta constante voy a decir que pertenece a la clase positiva, a esta clase positiva y si es menor si esa ecuación booleana a me da negativo pues va a pertenecer parte negativa tomando en cuenta el sesgo podemos llegar a esta ecuación que es la misma ecuación de la línea, hay que recordar que aquí $x$ es un vector que pertenece a este espacio y $w$ es otro vector que pertenece a este espacio todo su producto punto es un escalar. Sumado un escalar lo puedo comparar con otra escala entonces recordando un poquito la ecuación de la línea la ecuación de esta línea es $w \\cdot x + b = 0$ todo lo que sea mayor a 0 va a pertenecer de este lado del el plano y todo lo que sea menor a 0 va a pertenecer de este lado de libertad\n","\n","$w^Tx$ lo estoy utilizando con el producto punto para enfatizar un poco que esto es la proyección, este vector, es la proyección pero lo que yo quiero realmente es encontrar puntos que estén fuera de estos márgenes no simplemente que pasen el hiper plano sino que también estén alejados del hiper plano por cierta dimensión. \n","\n","Vamos a recordar la ecuación para calcular la distancia de un punto hacia una línea vale de un punto hacia un hiper plano entonces recordando un poquito para una línea vamos a verlo de esta manera para una línea $ax+by+c$ vale la distancia a un punto la distancia de la línea al punto  viene dada por esta misma ecuación $d(l,p)= \\frac{ax+by+c}{\\sqrt{a^2+b^2}}$ esta ecuación para un punto con coordenadas $(x,y)$ una línea con parámetros **a b** y **c** esta vez es la distancia, ojo aquí la distancia depende de los parámetros de mi línea, entonces yo podría poner una distancia ficticia a este margen que puedo llamarle un 1, esa distancia se va a escalar entonces no importa mucho el dígito que yo ponga por simplicidad voy a poner que en esta métrica esa distancia me va a valer 1.\n","\n","\n","![3](https://github.com/DataEngel/Data-Science-Machine-Learning-Articles/assets/63415652/c467a3ed-4287-467f-a3a4-d7341f861ad4)\n"],"metadata":{"id":"Y5Dk8C5E1isk"}},{"cell_type":"markdown","source":["Vamos a verlo en ecuaciones entonces la ecuación que nosotros teníamos esta de aquí la vamos a cambiar porque ya no quiero simplemente que esté de un lado del híper plano o del otro sino que también esté a cierto margen entonces en lugar de que sea cero que es el lugar geométrico donde vive esta línea vale el híper plano azul pues lo que voy a poner 1.  "],"metadata":{"id":"crWK1G9N16UI"}},{"cell_type":"markdown","source":["## Reglas de decision:\n","\n","Para muestras positivas se tiene que dar una razón mayor que uno, qué significa eso? que de mi hiper plano y de su margen derecho esta muestra va a estar para la derecha, si esta muestra es mayor que uno quiere decir que estoy fuera del margen que estoy más allá del margen y para una muestra negativa ocurrió exactamente lo contrario yo creo que esté a menos 1 de ese hiper plano, entonces estas ecuaciones se cumplen siempre que sepamos que las muestras son positivas o si son negativas. \n","\n","- Agreguemos una restricción la problema.\n","    - $w \\cdot{x_x+b \\geq{1}}$\n","    - $w\\cdot{x_-+b\\leq{-1}}$\n","\n","Una forma de generalizar esto es poniendo el valor deseado para cada muestra entonces el valor deseado me dice si quiero que sea parte de los positivos que me dé 1 si $x_1$ es una muestra positiva o que me dé menos 1 si $x_1$ es una muestra negativa, lo que estamos haciendo ahorita es simplemente una clasificación binaria entonces puedo generalizar estas dos ecuaciones con esta: \n","\n","- Juntamos las ecuaciones:\n","\n","$y_i= \\left\\{\n","\\begin{array}{ l }\n","1 \\:\\: Si \\:\\: x_i \\:\\: es \\:\\: + \\\\\n","-1 \\:\\: Si \\:\\: x_i \\:\\:es \\:\\: -\n","\\end{array}\n","\\right.$ -------->  $y_1(w\\cdot{x_i}+b)\\geq{1}$\n","\n","\n","Entonces para todos los puntos vale **x** van a cumplir esta manera ecuación si están fuera del margen, la idea de Vapnik es encontrar la línea con este margen bilateral tal que ningún punto caiga dentro de este margen, si hay un punto aquí hay que hacer el margen más pequeño, entonces hay que tratar de hacer el margen lo más grande posible tal que no haya ningún punto que esté dentro y eso va a ser condicionado por estos puntos que van a estar en las en los márgenes y de hecho a estos puntos reciben un nombre especial que son vectores soportes como se imaginarán pueda técnica significa máquinas de vector soporte entonces va a ser una máquina de estos vectores que tenemos que encontrar todo va a depender de estos vectores que van a estar en la periferia del margen bien entonces esta ecuación que hemos desarrollado hasta ahorita nos asegura que un punto de $(x,y)$ va a estar fuera del margen que queremos poner,.\n","\n","Ahora, qué pasa si yo quiero los vectores soportes no cualquier patrón sino los vectores soportes en el caso de los vectores soportes no solo se va a cumplir esta ecuación sino que se va a cumplir con la igualdad.  \n","\n","Entonces despejando la ecuación anterior voy a pasar el uno al otro lado con signo negativo vale y para las muestras que son los vectores soporte se va a cumplir en la igualdad vale es decir estos vectores que están en la periferia del margen van a cumplir esa igualdad y todos los que estén más allá van a cumplir la desigualdad.\n","\n","Entonces a partir de los parámetros de la línea hemos llegado a una ecuación que me detecta este esos vectores soportes vale todos los vectores que sean soporte van a cumplir esa ecuación de ahí. \n","\n","- Despejando la ecuación:\n","    - $y_i(w\\cdot{x_i}+b)\\geq1$\n","    - $y_i(w\\cdot{x_i}+b)-1\\geq0$\n","- En el caso especial de muestras x que estén en el margen.\n","    - $y_i(w\\cdot{x_i}+b)-1=0$"],"metadata":{"id":"LbGAKfOLFXLV"}},{"cell_type":"markdown","source":["## Longitud del margen.\n","\n","Vamos a preguntarnos ahora por otro lado cuál sería el tamaño del margen, yo lo que quiero es encontrar una línea cuyo margen sea lo más grande posible porque eso me va a hacer que esta separación sea la mayor entre mayor sea esta separación pues mejor va a generalizar nuestro algoritmo, todo lo que me interesa es sacar este cuánto mide este margen de aquí yo lo voy a hacer con una muestra, voy a imaginar una muestra que es negativa que voy a vivir aquí justo en medio y otra muestra que es positiva y que vive en el margen pues voy a agarrar dos vectores soporte, los vectores soportes son los vectores que viven en el margen del hiper plano, entonces esos vectores lo que voy a hacer es voy a proyectar los con el vector $w$ la proyección de un vector sobre otro pues es $x \\cdot { \\frac{w}{||w||} }$ es decir vamos a proyectar sobre el vector unitario $\\hat{w}$.\n","\n","Entonces esta proyección lo que me va a hacer es la sombra, este de aquí sería la sombra de $x$ menos 1 con w y si yo proyecto x más pues me va a dar otra sombra en la misma dirección $x_+\\hat{w}$ al proyectar con el vector unitario de w pues yo puedo encontrar esos dos vectores que van a intersecar en esta en esta dirección perpendicular a mí per plano y la diferencia entre esos dos me va a dar la distancia de ese hiper plano, entonces si yo hago $(x_+-x_-)\\cdot{\\frac{w}{||w||}}$ voy a obtener la distancia de ese hiper plano, el tamaño del margen.\n","\n","![4](https://github.com/DataEngel/Data-Science-Machine-Learning-Articles/assets/63415652/ed349734-4827-4ae7-9666-22be6955e2d4)\n"],"metadata":{"id":"pSgWQozWFXkb"}},{"cell_type":"markdown","source":["Vamos a elaborar un poquito más esto, supongamos que tenemos una muestra positiva y vamos a sustituirlo en la ecuación $y_i(w\\cdot{x_i}+b)-1=0$ que ya habíamos encontrado vale porque porque es un vector soporte pues yo puedo sustituir en esa ecuación y se va a cumplir entonces para una muestra positiva este $y_1$ va a ser igual a 1 sustituimos ahí la y la pasamos a un 1 y podemos multiplicar por las dos cosas y la vela podemos pasar hacia el otro lado un signo negativo $(1)(w\\cdot{x_+}+b)=1$ y me queda que $w\\cdot{x}=1-b$  \n","\n","Al contrario para una muestra negativa este valor deseado pues va y a ser un -1: \n","\n","- $y_i(w\\cdot{x_i}+b)-1=0$\n","\n","lo sustituimos este lo distribuimos en esta multiplicación \n","\n","- $(-1)(w\\cdot{x_-}+b)=1$\n","\n","Y  b la pasamos al otro lado, tiene signo positivo por menos sería negativo y al pasarlo al otro lado vuelve a ser positivo: \n","\n","- $-w\\cdot{x}=1+b$\n","\n","Y ojo aquí nos queda el signo negativo de lado izquierdo. \n","\n"," \n","\n","Ahora voy a distribuir este producto punto que tenemos en esta ecuación y voy a hacer los cambios nuevos que acabo de encontrar, estos dos: \n","\n","- $w\\cdot{x}=1-b$\n","- $-w\\cdot{x}=1+b$\n","\n"," El producto punto es distributivo y aparte es conmutativo entonces, puedo llegar a esta ecuación de aquí a la segunda y ya me queda términos de w por equis positiva y w por equis negativa:\n","\n","- $d = \\frac{w}{||w||}(w \\cdot{x}-w\\cdot{x_-})$\n","\n","que son cosas que ya encontré en las ecuaciones anteriores sustituyó 1 - b y 1 + b: \n","\n","- $d = \\frac{w}{||w||}((1-b)+(1+b))$\n","\n","como pueden ver se van a ir se van a eliminar y me va a quedar nada más un 2:\n","\n","- $d = \\frac{2}{||w||}$\n","\n","Entonces el margen separador de una línea va a depender únicamente de sus parámetros vale no depende de x sino que depende solamente de sus parámetros entonces eso está muy padre porque ustedes pueden agarrar cualquier línea los puntos para acá y es a la propia línea les va a dar los parámetros el tamaño del margen. Siempre vamos a tratar de encontrar la línea con el margen más grande veamos el tamaño del margen y sería esa distancia.\n","\n","![Untitled](https://github.com/DataEngel/Data-Science-Machine-Learning-Articles/assets/63415652/c22f1a8b-1fdc-465f-a6a4-6bd5f678abfc)\n","\n","entonces $d = \\frac{2}{||w||}$ sería la distancia de cada lado $\\frac{1}{||w||}$ bueno cuando encontremos la línea que tenga el margen lo más grande posible esa va a ser la línea óptima, que quiere decir que encontré los vectores patrón este adecuados bien entonces vamos a plantear esto como un problema de optimización vale lo que yo voy a querer es maximizar el margen vale la distancia del margen es decir que voy a maximizar 2 entre la norma de w moviendo w. entonces se vuelve un problema de optimización yo quiero encontrar la mejor línea y la mejor línea la voy a encontrar de esta manera."],"metadata":{"id":"Y2qN5Wc5FXm4"}},{"cell_type":"markdown","source":["## Problema de optimización:\n","\n","Como es el problema está un poco raro vamos a cambiarlo a otro problema yo entonces hablamos de problemas de optimización los los valores constantes no nos afectan mucho entonces yo puedo pasar de este problema de optimización a este otro de acá porque al maximizar la segunda expresión también se va a maximizar la primera\n","\n","$1.-\\max_{w}\\frac{2}{||w||}$——> $2.-\\max_{w}\\frac{1}{||w||}$\n","\n","Esto en problemas de optimización le llamamos problemas equivalentes entonces cuando un problema no queda con la ecuación muy adecuada para nosotros lo que podemos hacer es cambiarlo un problema equivalente como hacemos eso? Siempre que se maximice un problema aseguramos la maximización del otro o la minimización del otro no quieran ver pues eso quiere decir que estos problemas son equivalentes en optimización\n","\n","entonces en lugar de maximizar es más común el minimiza y en lugar de maximizar esta fracción 2 pues yo puedo minimizar esto de aquí  de hecho, fíjense un poquito que al al maximizar es decir esta fracción se va a ser más grande cuando la norma de w sea muy pequeña vale entonces maximizar la fracción 2 es lo mismo que minimizar la expresión 3 \n","\n"," $1.-\\max_{w}\\frac{2}{||w||}$ ——> $2.-\\max_{w}\\frac{1}{||w||}$——> $3.-\\min_{w}||w||$\n","\n","pero bueno esta expresión nos sigue sin quedar muy de bien entonces la vamos a transformar la fracción 3 a esto de acá de hecho para esto recuerden que la norma de un vector cuando no le ponemos la norma, estamos pensando en una norma 2 y esa norma 2, si la elevó al cuadrado me va a quedar lo mismo que $||w||^2=w^Tw$ lo que estoy haciendo es elevando la norma al cuadrado y dividiendo la entre un medio:\n","\n","$4.-\\max_{w}\\frac{1}{2}w^Tw$\n","\n","por que puedo hacer esto? porque al minimizar la expresión 4 también se va a minimizar la expresión 3. \n","\n","En resumen, si yo quiero minimizar la expresión 4, también se va a minimizar la expresión 3 y de igual forma se va a maximizar la 2 y en consecuencia el problema original, porque son equivalentes.\n","\n","Esto va a quedar como nuestro problema de optimización más bien hay todavía un problemita que es que este problema de optimización no viene solo viene con algunas restricciones: \n","\n","- $y_i(w\\cdot{x_i}+b)\\geq1$\n","- $y_i(w\\cdot{x_i}+b)-1\\geq0$\n","\n","Estas restricciones que ya habíamos puesto por aquí, entonces yo tengo que optimizar ese margen pero también tengo que asegurar que haya unos vectores soportes que caigan en el margen, entonces aquí estamos utilizando la información de los datos más la información del híper plan, entonces nuestro problema de optimización va a pasar a ser este de aquí vale va a ser un problema de minimización con restricciones \n","\n","$\\max_{w}\\frac{1}{2}w^Tw$    Sujeto a:   $y_i(w\\cdot{x_i}+b)-1\\geq0$     Para todos los vectores de soporte. \n","\n","Voy a minimizar esto vale sujeto a estas restricciones de aquí donde las $x_i$ van a ser los vectores soportes es decir las $x_i$ van a ser los vectores que caigan ahí en el margen\n","\n","![1](https://github.com/DataEngel/Data-Science-Machine-Learning-Articles/assets/63415652/536ee9df-6649-4774-89d6-ddceb48a46dd)\n","\n","entonces para cada híper plano que yo pueda crear voy a buscar esos vectores soportes que hacen esa ecuación ser"],"metadata":{"id":"o6ZOmbPlFXpp"}},{"cell_type":"markdown","source":["## Multiplicadores de Lagrange\n","\n","Ahora que tenemos un problema de minimización con restricciones para esto solemos usar una herramienta que le llamamos los multiplicadores de la Lagramge. Recordando un poquito de este tema cuando tenemos un problema de optimización y aparte de ese problema tenemos restricciones a este tipo de restricciones le llamamos restricciones de igualdad también hay restricciones de inequidades, pues yo tengo n restricciones de igualdad \n","\n","- Para un problema de optimización:\n","\n","  * $\\min_{x}f(x)$ \n","\n","- Sujeto a:\n","\n","  * $g_i=0 \\:\\: para \\:\\: i=1,2,...,n$\n","    \n"],"metadata":{"id":"mhob4ogknwxE"}},{"cell_type":"code","source":[],"metadata":{"id":"wKRP4P0wn0LX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["$\\min_{x}f(x)$"],"metadata":{"id":"SfEoXC4eoa-o"}},{"cell_type":"code","source":[],"metadata":{"id":"hybRq2oEodCU"},"execution_count":null,"outputs":[]}]}