{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMFOl2/e7fg8erhfRAPmpj+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# SMV Theory\n","\n","## Contexto historico:\n","\n","Fueron desarrolladas principalmente por Vladimir Vapnik en los años 90s en la unión soviética, quien fue por mucho tiempo profesor en una de las universidades soviética hasta que en el año 1990 emigró a estados unidos donde trabajó en AT&T en este periodo en la década de los 90s cuando desarrolló esta idea de la máquina de soporte vectorial que se volvió muy importante no sólo la técnica en sí sino también toda la teoría que se desarrolló alrededor de esta técnica entonces aparte de la SVM es importante conocer que también a partir de ella se desarrolló la teoría de Vapnik-Chervonenkis del aprendizaje estadístico aquí pueden ver la imagen a Vapnik leyendo su propio libro sobre la teoría de aprendizaje estadístico."],"metadata":{"id":"_shRU9FsR0Dg"}},{"cell_type":"markdown","source":["![Screenshot 2023-05-15 235357](https://github.com/DataEngel/scaffold/assets/63415652/161651d9-5630-405e-a175-d267bcd57764)"],"metadata":{"id":"sr0Zw-ZYINA4"}},{"cell_type":"markdown","source":["Las SVM la pueden encontrar para estos tres propósitos: \n","\n","- Clasificación.\n","- Regresión.\n","- Clustering.\n","\n","El principal es el problema de clasificación lo vamos a ver de esta manera para no tener que ver cada uno de estos problemas y lo vamos a ver como un problema de clasificador pero hay algunas extensiones para volverlo un regresor con truco muy parecido a lo que hace cada KNN y hay otro truco para convertirlo a un algoritmo de clustering que es no supervisado que fue publicado por Vapnik y otro científico en el 2001, pero solo vamos a ver el clasificador para entender las inspiraciones de la SVM y vamos a partir con un problema de clasificación."],"metadata":{"id":"BtXyfKZqHeN5"}},{"cell_type":"markdown","source":["## Hiperplano optimo.\n","\n","Lo que buscamos es maximizar la distancia de nuestro hiperplano, a esto le llaman el método de la calle porque parece un poco una calle donde tenemos la línea del medio que divide los carriles y los bordes de nuestra carretera, entonces Vapnik tuvo la idea de que la línea óptima que separa estas dos clases y por lo tanto la que tiene mejor generalización, es decir la que lo hace mejor para puntos patrones que nunca ha visto va a ser esta línea de aquí, donde el margen sea el máximo posible se busca maximizar el margen de aquí. Entonces, vamos a desarrollar con esta idea que es muy sencilla las matemáticas que Vapnik creó.\n","\n","\n","![1](https://github.com/DataEngel/Data-Science-Machine-Learning-Articles/assets/63415652/d8678645-a821-4db8-b564-6a728dc09aeb)"],"metadata":{"id":"3CNm0l6JOCbw"}},{"cell_type":"markdown","source":["Esta línea tiene parámetros $w$ entonces voy a poner $w$ como un vector que va a ser $w_1$ y $w_2$ y también tiene parámetros $b$, si vemos $w$ está como un vector va a ser un vector, entonces voy a estirar esta línea que va a ser perpendicular a nuestra línea eso se cumple siempre si yo tengo los parámetros $w$ los parámetros no independientes de una línea de un híper plano me van a formar un vector que va a apuntar de manera normal a intercalar el hyperplane, esto es muy importante porque vamos a usar ese vector $w$ que acabamos de escribir para describir cualquier otro punto.   \n","\n","Entonces si yo agarro otro punto vamos a pensar un punto por aquí le voy a llamar $x$ pensándolo que es un vector, estas líneas son rectas acabo de dibujar vamos a tener un vector $w$ y un vector $x$, cómo puedo describir ese vector $x$ con respecto a $w$.\n","\n","![Screenshot 2023-05-18 225625](https://github.com/DataEngel/Data-Science-Machine-Learning-Articles/assets/63415652/2045654b-2699-4d25-8f08-578dbea6bb49)"],"metadata":{"id":"b78aJ-ZlRPwP"}},{"cell_type":"markdown","source":["Pues lo puedo describir con la proyección es decir la sombra de este vector sobre $w$ vale y bueno yo sé que esa proyección realmente sería $x$ producto punto de $w$ entre la norma de $w$. Esta proyección va pintar de otro conocido esta proyección de aquí entonces muy importante porque los productos puntos nos van a ayudar a proyectar cualquier punto en dirección hacia el hiper plano en una dirección normal al hiper plano.  \n","\n","### Proyectando una muestra. \n","\n","![2](https://github.com/DataEngel/Data-Science-Machine-Learning-Articles/assets/63415652/3410cb28-e87d-4c0e-950a-29b798d5c43d)"],"metadata":{"id":"rqwR-tME06Vw"}},{"cell_type":"code","source":[],"metadata":{"id":"sDhMkc0d068j"},"execution_count":null,"outputs":[]}]}